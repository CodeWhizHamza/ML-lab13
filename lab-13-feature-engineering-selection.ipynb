{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|key|value|\n",
    "|----|-----|\n",
    "|Name:|M.Hamza|\n",
    "|CMS ID:|407251|\n",
    "|Course:|Machine Learning CS-470|\n",
    "|Lab:|13|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:28.103682Z",
     "iopub.status.busy": "2024-12-20T11:47:28.103266Z",
     "iopub.status.idle": "2024-12-20T11:47:28.581855Z",
     "shell.execute_reply": "2024-12-20T11:47:28.580619Z",
     "shell.execute_reply.started": "2024-12-20T11:47:28.103625Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands On Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:28.583775Z",
     "iopub.status.busy": "2024-12-20T11:47:28.583286Z",
     "iopub.status.idle": "2024-12-20T11:47:29.866126Z",
     "shell.execute_reply": "2024-12-20T11:47:29.864952Z",
     "shell.execute_reply.started": "2024-12-20T11:47:28.583741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Example Dataset\n",
    "data = {'Age': [25, None, 35, 29], 'Salary': [50000, 54000, None, 58000]}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Imputation with Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:29.870310Z",
     "iopub.status.busy": "2024-12-20T11:47:29.869348Z",
     "iopub.status.idle": "2024-12-20T11:47:29.900227Z",
     "shell.execute_reply": "2024-12-20T11:47:29.898832Z",
     "shell.execute_reply.started": "2024-12-20T11:47:29.870267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Numerical Imputation with Mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['Age'] = imputer.fit_transform(df[['Age']])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Imputation (replace NaN with 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:29.902293Z",
     "iopub.status.busy": "2024-12-20T11:47:29.901847Z",
     "iopub.status.idle": "2024-12-20T11:47:29.925379Z",
     "shell.execute_reply": "2024-12-20T11:47:29.923900Z",
     "shell.execute_reply.started": "2024-12-20T11:47:29.902232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Categorical Imputation (replace NaN with 'Unknown')\n",
    "df['Salary'] = df['Salary'].fillna(df['Salary'].mean())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:29.927717Z",
     "iopub.status.busy": "2024-12-20T11:47:29.927166Z",
     "iopub.status.idle": "2024-12-20T11:47:29.952861Z",
     "shell.execute_reply": "2024-12-20T11:47:29.951552Z",
     "shell.execute_reply.started": "2024-12-20T11:47:29.927654Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example Dataset\n",
    "data = {'City': ['London', 'Paris', 'Berlin']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# One-Hot Encoding\n",
    "one_hot = pd.get_dummies(df['City'])\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:29.954916Z",
     "iopub.status.busy": "2024-12-20T11:47:29.954463Z",
     "iopub.status.idle": "2024-12-20T11:47:29.976253Z",
     "shell.execute_reply": "2024-12-20T11:47:29.974991Z",
     "shell.execute_reply.started": "2024-12-20T11:47:29.954877Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "df['City_Label'] = le.fit_transform(df['City'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:29.978064Z",
     "iopub.status.busy": "2024-12-20T11:47:29.977584Z",
     "iopub.status.idle": "2024-12-20T11:47:29.992604Z",
     "shell.execute_reply": "2024-12-20T11:47:29.990925Z",
     "shell.execute_reply.started": "2024-12-20T11:47:29.978005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Example Dataset\n",
    "data = {'Age': [25, 35, 29], 'Salary': [50000, 54000, 58000]}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:29.994845Z",
     "iopub.status.busy": "2024-12-20T11:47:29.994402Z",
     "iopub.status.idle": "2024-12-20T11:47:30.024797Z",
     "shell.execute_reply": "2024-12-20T11:47:30.023365Z",
     "shell.execute_reply.started": "2024-12-20T11:47:29.994793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df[['Age', 'Salary']] = scaler.fit_transform(df[['Age', 'Salary']])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:30.030735Z",
     "iopub.status.busy": "2024-12-20T11:47:30.030284Z",
     "iopub.status.idle": "2024-12-20T11:47:30.054684Z",
     "shell.execute_reply": "2024-12-20T11:47:30.053098Z",
     "shell.execute_reply.started": "2024-12-20T11:47:30.030696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "normalizer = MinMaxScaler()\n",
    "df[['Age', 'Salary']] = normalizer.fit_transform(df[['Age', 'Salary']])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:30.056642Z",
     "iopub.status.busy": "2024-12-20T11:47:30.056026Z",
     "iopub.status.idle": "2024-12-20T11:47:30.080782Z",
     "shell.execute_reply": "2024-12-20T11:47:30.079161Z",
     "shell.execute_reply.started": "2024-12-20T11:47:30.056586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = {'Bedrooms': [3, 4, 2], 'House_Size': [1000, 1200, 800]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Interaction Feature\n",
    "df['Rooms_per_Square_Meter'] = df['Bedrooms'] / df['House_Size']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:30.082934Z",
     "iopub.status.busy": "2024-12-20T11:47:30.082575Z",
     "iopub.status.idle": "2024-12-20T11:47:30.104694Z",
     "shell.execute_reply": "2024-12-20T11:47:30.103350Z",
     "shell.execute_reply.started": "2024-12-20T11:47:30.082903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = {'Income': [1000, 10000, 50000, 100000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df['Log_Income'] = np.log1p(df['Income'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:30.106729Z",
     "iopub.status.busy": "2024-12-20T11:47:30.106307Z",
     "iopub.status.idle": "2024-12-20T11:47:30.127877Z",
     "shell.execute_reply": "2024-12-20T11:47:30.126128Z",
     "shell.execute_reply.started": "2024-12-20T11:47:30.106681Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Example Dataset\n",
    "data = {'Feature': [2, 3, 4]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "polynomial_features = poly.fit_transform(df[['Feature']])\n",
    "polynomial_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:30.130131Z",
     "iopub.status.busy": "2024-12-20T11:47:30.129585Z",
     "iopub.status.idle": "2024-12-20T11:47:30.159052Z",
     "shell.execute_reply": "2024-12-20T11:47:30.157587Z",
     "shell.execute_reply.started": "2024-12-20T11:47:30.130080Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example Dataset\n",
    "data = {'Age': [25, 35, 29, 50, 60]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Binning\n",
    "bins = [0, 30, 50, 100]\n",
    "labels = ['Young', 'Middle-Aged', 'Senior']\n",
    "df['Age_Group'] = pd.cut(df['Age'], bins=bins, labels=labels)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands On With Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:30.161412Z",
     "iopub.status.busy": "2024-12-20T11:47:30.160971Z",
     "iopub.status.idle": "2024-12-20T11:47:31.049041Z",
     "shell.execute_reply": "2024-12-20T11:47:31.047879Z",
     "shell.execute_reply.started": "2024-12-20T11:47:30.161350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"students_performance.csv\")\n",
    "\n",
    "# Filter numeric columns\n",
    "numeric_data = data.select_dtypes(include=[\"number\"])\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = numeric_data.corr()\n",
    "\n",
    "# Visualize the matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Feature with high correlation (> 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:31.051146Z",
     "iopub.status.busy": "2024-12-20T11:47:31.050660Z",
     "iopub.status.idle": "2024-12-20T11:47:31.061011Z",
     "shell.execute_reply": "2024-12-20T11:47:31.059662Z",
     "shell.execute_reply.started": "2024-12-20T11:47:31.051096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.85\n",
    "high_corr_pairs = [\n",
    "    (col1, col2)\n",
    "    for col1 in correlation_matrix.columns\n",
    "        for col2 in correlation_matrix.columns\n",
    "            if abs(correlation_matrix[col1][col2]) > threshold and col1 != col2\n",
    "]\n",
    "# Output the highly correlated pairs\n",
    "print(\"Highly correlated pairs (|correlation| > 0.85):\")\n",
    "for col1, col2 in high_corr_pairs:\n",
    "    print(f\"{col1} - {col2}: {correlation_matrix[col1][col2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:31.063392Z",
     "iopub.status.busy": "2024-12-20T11:47:31.062944Z",
     "iopub.status.idle": "2024-12-20T11:47:31.121278Z",
     "shell.execute_reply": "2024-12-20T11:47:31.120220Z",
     "shell.execute_reply.started": "2024-12-20T11:47:31.063309Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Check column names\n",
    "print(\"Dataset columns:\", data.columns)\n",
    "\n",
    "# Ensure 'passed_all' exists (modify this logic as per your requirement)\n",
    "if \"passed_all\" not in data.columns:\n",
    "    # Example logic: Passed all exams if scores are >= 50\n",
    "    data[\"passed_all\"] = (data[\"math score\"] >= 50) & (data[\"reading score\"] >= 50) & (data[\"writing score\"] >= 50)\n",
    "\n",
    "# Encode categorical columns using one-hot encoding\n",
    "categorical_columns = ['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course']\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Define features and target\n",
    "X = data_encoded.drop(columns=[\"passed_all\"])\n",
    "y = data_encoded[\"passed_all\"]\n",
    "\n",
    "# Apply ANOVA F-test\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# Display selected feature scores\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"Selected Features:\", list(selected_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:31.123679Z",
     "iopub.status.busy": "2024-12-20T11:47:31.123163Z",
     "iopub.status.idle": "2024-12-20T11:47:33.436454Z",
     "shell.execute_reply": "2024-12-20T11:47:33.435220Z",
     "shell.execute_reply.started": "2024-12-20T11:47:31.123628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Apply RFE\n",
    "rfe_selector = RFE(estimator=model, n_features_to_select=5, step=1)\n",
    "X_rfe = rfe_selector.fit_transform(X, y)\n",
    "\n",
    "# Get selected features\n",
    "selected_rfe_features = X.columns[rfe_selector.support_]\n",
    "print(\"RFE Selected Features:\", list(selected_rfe_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance from tree based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:33.438301Z",
     "iopub.status.busy": "2024-12-20T11:47:33.437836Z",
     "iopub.status.idle": "2024-12-20T11:47:33.948301Z",
     "shell.execute_reply": "2024-12-20T11:47:33.947020Z",
     "shell.execute_reply.started": "2024-12-20T11:47:33.438250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Train Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Plot feature importance\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.barh(X.columns[indices], importances[indices])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance from Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:33.950831Z",
     "iopub.status.busy": "2024-12-20T11:47:33.950248Z",
     "iopub.status.idle": "2024-12-20T11:47:33.970775Z",
     "shell.execute_reply": "2024-12-20T11:47:33.969611Z",
     "shell.execute_reply.started": "2024-12-20T11:47:33.950791Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Apply Lasso for feature selection\n",
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Select features with non-zero coefficients\n",
    "lasso_selector = SelectFromModel(lasso, prefit=True)\n",
    "X_lasso = lasso_selector.transform(X)\n",
    "\n",
    "# Get selected features\n",
    "selected_lasso_features = X.columns[lasso_selector.get_support()]\n",
    "print(\"Lasso Selected Features:\", list(selected_lasso_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:33.973839Z",
     "iopub.status.busy": "2024-12-20T11:47:33.972446Z",
     "iopub.status.idle": "2024-12-20T11:47:34.569702Z",
     "shell.execute_reply": "2024-12-20T11:47:34.568452Z",
     "shell.execute_reply.started": "2024-12-20T11:47:33.973640Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "features = X.columns\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(features, importances, color=\"skyblue\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance (Random Forest)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shap Summary Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:34.572018Z",
     "iopub.status.busy": "2024-12-20T11:47:34.571667Z",
     "iopub.status.idle": "2024-12-20T11:47:41.910189Z",
     "shell.execute_reply": "2024-12-20T11:47:41.909023Z",
     "shell.execute_reply.started": "2024-12-20T11:47:34.571983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Explain model predictions\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# SHAP summary plot\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:41.912163Z",
     "iopub.status.busy": "2024-12-20T11:47:41.911652Z",
     "iopub.status.idle": "2024-12-20T11:47:43.528127Z",
     "shell.execute_reply": "2024-12-20T11:47:43.527013Z",
     "shell.execute_reply.started": "2024-12-20T11:47:41.912128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Calculate permutation importance\n",
    "perm_importance = permutation_importance(model, X, y, scoring=\"accuracy\")\n",
    "\n",
    "# Plot\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "plt.barh(features[sorted_idx], perm_importance.importances_mean[sorted_idx], \n",
    "color=\"lightcoral\")\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Permutation Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:43.530526Z",
     "iopub.status.busy": "2024-12-20T11:47:43.529671Z",
     "iopub.status.idle": "2024-12-20T11:47:44.025526Z",
     "shell.execute_reply": "2024-12-20T11:47:44.024399Z",
     "shell.execute_reply.started": "2024-12-20T11:47:43.530450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('archive/Weather Training Data.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:44.027429Z",
     "iopub.status.busy": "2024-12-20T11:47:44.027049Z",
     "iopub.status.idle": "2024-12-20T11:47:44.060374Z",
     "shell.execute_reply": "2024-12-20T11:47:44.059167Z",
     "shell.execute_reply.started": "2024-12-20T11:47:44.027396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns=['row ID'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:44.062239Z",
     "iopub.status.busy": "2024-12-20T11:47:44.061889Z",
     "iopub.status.idle": "2024-12-20T11:47:44.068557Z",
     "shell.execute_reply": "2024-12-20T11:47:44.067483Z",
     "shell.execute_reply.started": "2024-12-20T11:47:44.062205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:44.070252Z",
     "iopub.status.busy": "2024-12-20T11:47:44.069923Z",
     "iopub.status.idle": "2024-12-20T11:47:44.117730Z",
     "shell.execute_reply": "2024-12-20T11:47:44.116476Z",
     "shell.execute_reply.started": "2024-12-20T11:47:44.070220Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as we can see, we have some features with zero missing values. some with under 10k and some with above 10k.\n",
    "\n",
    "Features that have missing values less than 10k, we will be imputing those values. For the remaining values, I will train a random forest model to predict them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:44.122928Z",
     "iopub.status.busy": "2024-12-20T11:47:44.122477Z",
     "iopub.status.idle": "2024-12-20T11:47:44.132717Z",
     "shell.execute_reply": "2024-12-20T11:47:44.131637Z",
     "shell.execute_reply.started": "2024-12-20T11:47:44.122880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "okay_features = ['Location', 'RainTomorrow']\n",
    "cols_with_missing_values = [col for col in dataset.columns if col not in okay_features]\n",
    "over_10k = ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm']\n",
    "under_10k = [v for v in dataset.columns if v not in okay_features and v not in over_10k]\n",
    "under_10k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove rows with all missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:44.134295Z",
     "iopub.status.busy": "2024-12-20T11:47:44.133961Z",
     "iopub.status.idle": "2024-12-20T11:47:44.192365Z",
     "shell.execute_reply": "2024-12-20T11:47:44.191196Z",
     "shell.execute_reply.started": "2024-12-20T11:47:44.134240Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(dataset.shape)\n",
    "dataset = dataset[~dataset.isna()[cols_with_missing_values].all(axis=1)]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove rows that has missing values more than or equal to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:44.194468Z",
     "iopub.status.busy": "2024-12-20T11:47:44.194030Z",
     "iopub.status.idle": "2024-12-20T11:47:44.253882Z",
     "shell.execute_reply": "2024-12-20T11:47:44.252792Z",
     "shell.execute_reply.started": "2024-12-20T11:47:44.194430Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# removing all the records that has missing values of 5 or more\n",
    "dataset = dataset.iloc[(dataset.isna()[cols_with_missing_values].sum(axis=1) < 5).values, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:44.255561Z",
     "iopub.status.busy": "2024-12-20T11:47:44.255205Z",
     "iopub.status.idle": "2024-12-20T11:47:44.287523Z",
     "shell.execute_reply": "2024-12-20T11:47:44.286163Z",
     "shell.execute_reply.started": "2024-12-20T11:47:44.255527Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Checking the missing values again\n",
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing under 10k features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:44.289750Z",
     "iopub.status.busy": "2024-12-20T11:47:44.289199Z",
     "iopub.status.idle": "2024-12-20T11:47:44.406834Z",
     "shell.execute_reply": "2024-12-20T11:47:44.405651Z",
     "shell.execute_reply.started": "2024-12-20T11:47:44.289700Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for col in under_10k:\n",
    "    if dataset[col].dtype == 'float64':\n",
    "        dataset[col] = dataset[col].fillna(dataset[col].median())\n",
    "    else:\n",
    "        dataset[col] = dataset[col].fillna(dataset[col].mode()[0])\n",
    "\n",
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing features with over 10k values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:44.408471Z",
     "iopub.status.busy": "2024-12-20T11:47:44.408155Z",
     "iopub.status.idle": "2024-12-20T11:47:44.595939Z",
     "shell.execute_reply": "2024-12-20T11:47:44.594741Z",
     "shell.execute_reply.started": "2024-12-20T11:47:44.408440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset[~(dataset[over_10k].isna())].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:44.597850Z",
     "iopub.status.busy": "2024-12-20T11:47:44.597440Z",
     "iopub.status.idle": "2024-12-20T11:47:44.639209Z",
     "shell.execute_reply": "2024-12-20T11:47:44.638041Z",
     "shell.execute_reply.started": "2024-12-20T11:47:44.597816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm']\n",
    "dataset = dataset[~(dataset['Evaporation'].isna() & dataset['Sunshine'].isna() & dataset['Cloud9am'].isna() & dataset['Cloud3pm'].isna())]\n",
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:44.641196Z",
     "iopub.status.busy": "2024-12-20T11:47:44.640750Z",
     "iopub.status.idle": "2024-12-20T11:47:44.648869Z",
     "shell.execute_reply": "2024-12-20T11:47:44.647753Z",
     "shell.execute_reply.started": "2024-12-20T11:47:44.641147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing the remaining values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:44.650743Z",
     "iopub.status.busy": "2024-12-20T11:47:44.650362Z",
     "iopub.status.idle": "2024-12-20T11:47:44.693841Z",
     "shell.execute_reply": "2024-12-20T11:47:44.692432Z",
     "shell.execute_reply.started": "2024-12-20T11:47:44.650708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for col in over_10k:\n",
    "    if dataset[col].dtype == 'float64':\n",
    "        dataset[col] = dataset[col].fillna(dataset[col].median())\n",
    "    else:\n",
    "        dataset[col] = dataset[col].fillna(dataset[col].mode()[0])\n",
    "\n",
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:44.695625Z",
     "iopub.status.busy": "2024-12-20T11:47:44.695148Z",
     "iopub.status.idle": "2024-12-20T11:47:44.753940Z",
     "shell.execute_reply": "2024-12-20T11:47:44.752623Z",
     "shell.execute_reply.started": "2024-12-20T11:47:44.695565Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "categorical_columns = [col for col in dataset.columns if dataset[col].dtype == 'object']\n",
    "dataset = pd.get_dummies(\n",
    "    dataset, \n",
    "    columns=categorical_columns,\n",
    "    drop_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:47:44.755694Z",
     "iopub.status.busy": "2024-12-20T11:47:44.755310Z",
     "iopub.status.idle": "2024-12-20T11:47:44.763753Z",
     "shell.execute_reply": "2024-12-20T11:47:44.762327Z",
     "shell.execute_reply.started": "2024-12-20T11:47:44.755658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:56:28.782216Z",
     "iopub.status.busy": "2024-12-20T11:56:28.781424Z",
     "iopub.status.idle": "2024-12-20T11:56:28.814931Z",
     "shell.execute_reply": "2024-12-20T11:56:28.813727Z",
     "shell.execute_reply.started": "2024-12-20T11:56:28.782153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T11:59:55.520109Z",
     "iopub.status.busy": "2024-12-20T11:59:55.519733Z",
     "iopub.status.idle": "2024-12-20T11:59:55.549557Z",
     "shell.execute_reply": "2024-12-20T11:59:55.548399Z",
     "shell.execute_reply.started": "2024-12-20T11:59:55.520078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset['MaxTemp*Evaporation'] = dataset['MaxTemp'] * dataset['Evaporation']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "numerical_columns = [col for col in dataset.columns if dataset[col].dtype == 'float64']\n",
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[numerical_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We standardize the numerical columns\n",
    "scaler = StandardScaler()\n",
    "dataset[numerical_columns] = scaler.fit_transform(dataset[numerical_columns])\n",
    "dataset[numerical_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the correlation matrix of numerical columns and the target variable\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(dataset[numerical_columns + ['RainTomorrow']].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highly_correlated_features = []\n",
    "correlation_matrix = dataset[numerical_columns].corr()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.85:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            highly_correlated_features.append(colname)\n",
    "\n",
    "highly_correlated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns=highly_correlated_features)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use recursive feature elimination with cross-validation to select the best features\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = dataset.drop(columns=['RainTomorrow'])\n",
    "y = dataset['RainTomorrow']\n",
    "\n",
    "# use 50% of the data for training data\n",
    "X = X.sample(frac=0.5, random_state=42)\n",
    "y = y.loc[X.index]\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "selector = RFECV(model, step=1, cv=3, n_jobs=-1)\n",
    "selector = selector.fit(X, y)\n",
    "\n",
    "selected_features = X.columns[selector.support_]\n",
    "\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features that were not selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(X.columns) - set(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature importances\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::]\n",
    "\n",
    "plt.figure(figsize=(10, 18))\n",
    "# feature importance and the importance of the feature infront of it\n",
    "plt.barh(X.columns[indices], importances[indices])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance from Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the best features\n",
    "X_org = dataset[selected_features]\n",
    "y_org = dataset['RainTomorrow']\n",
    "\n",
    "X_org.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.hist(y_org)\n",
    "plt.xlabel('RainTomorrow')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('RainTomorrow Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusable function for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "    plot_confusion_matrix(y, y_pred)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=[\"No Rain\", \"Rain\"], yticklabels=[\"No Rain\", \"Rain\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_best_params(model, param_grid, X, y):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=1, scoring='f1')\n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search.best_params_\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(model, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Training Performance:\")\n",
    "    evaluate_model(model, X_train, y_train)\n",
    "\n",
    "    print(\"\\n\\nTest Performance:\")\n",
    "    evaluate_model(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"Simple Logistic Regression Model:\")\n",
    "# give more weight to 1 class\n",
    "model = LogisticRegression(class_weight={0: 1, 1: 5}, random_state=42)\n",
    "train_and_evaluate_model(model, X_org, y_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Logistic Regression with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "model = LogisticRegression(random_state=42, class_weight={0: 1, 1: 5})\n",
    "best_params = get_best_params(model, param_grid, X_org, y_org)\n",
    "\n",
    "print(\"Best Parameters for Logistic Regression:\", best_params)\n",
    "\n",
    "model = LogisticRegression(**best_params, random_state=42, class_weight={0: 1, 1: 5})\n",
    "print(\"\\nLogistic Regression Model with Best Parameters:\")\n",
    "train_and_evaluate_model(model, X_org, y_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"Simple Random Forest Model:\")\n",
    "model = RandomForestClassifier(random_state=42, min_samples_split=5, n_estimators=100, max_depth=10, class_weight={0: 1, 1: 5})\n",
    "train_and_evaluate_model(model, X_org, y_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Random Forest with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, class_weight={0: 1, 1: 5})\n",
    "best_params = get_best_params(model, param_grid, X_org, y_org)\n",
    "\n",
    "print(\"Best Parameters for Random Forest:\", best_params)\n",
    "\n",
    "model = RandomForestClassifier(**best_params, random_state=42, class_weight={0: 1, 1: 5})\n",
    "print(\"\\nRandom Forest Model with Best Parameters:\")\n",
    "train_and_evaluate_model(model, X_org, y_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair plot of the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dataset = dataset[list(set(numerical_columns) & set(selected_features)) + ['RainTomorrow']]\n",
    "sns.pairplot(selected_dataset, hue='RainTomorrow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shap to explain the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap to explain the model predictions\n",
    "import shap\n",
    "\n",
    "model = RandomForestClassifier(**best_params, random_state=42, class_weight={0: 1, 1: 5}, n_jobs=-1)\n",
    "model.fit(X_org, y_org)\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_org)\n",
    "\n",
    "shap.summary_plot(shap_values[1], X_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = selected_dataset.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, I have performed feature engineering and feature selection on the dataset. I have imputed the missing values, encoded the categorical columns, created interaction features, standardized the data, and selected the best features. I have trained a logistic regression and random forest model on the dataset. I have also visualized the class distribution, pair plot of the selected features, and correlation heatmap. I have used SHAP to explain the model.\n",
    "\n",
    "Other than that, I have also performed a mini challenge where I have removed unnecessary features, handled missing values, and trained a model on the dataset."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2414370,
     "sourceId": 4080187,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6321004,
     "sourceId": 10224551,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
